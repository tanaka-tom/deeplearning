---
layout: post
title: "自然言語処理を学ぼう #3 word2vec 理論編"
date: 2018-07-28 08:00:00 +0900
categories: natural
youtube: "Lj7LzB6EFJI"
---

## 目的

前回のところで Bag-of-Words を扱って、映画の類似あらすじ予測を行ったが精度が低かったので、別の手法を試していく。
今回は、word2vec について。

## word2vec

- 単語の意味をベクトル化することができる
- 数十万次元のベクトルを特徴量の数百次元にすることが可能

### word2vec でできること

以前田中 TOM で word2vec を紹介した動画
[word2vec：Wikipedia を機械学習して単語をベクトル化！？「本田にとってのサッカーはイチローにとっての何？」に答えちゃった](https://youtu.be/Uvgth7oQECM)

- ベクトルの計算を行える
  - Q: 王様 - 男 + 女 = ?
  - A: 女王
  - みたいな計算を単語ですることができる
  - 元の文章のデータに答えは依存する
- 類似度を見つけることができる
  - Q: ラーメンに関連(類似)している単語は？
  - A: 餃子、うどん
  - のように類似度の高いベクトルを探して似た単語がわかる

## 単語のベクトルを作るためのロジック

word2vec には２つのモデルが存在する

- Countinuous Bag-of-Words(CBOW)
  - 周りの単語から挟まれている単語を推測
- Skip-Gram Model
  - 1 単語から周りの単語を推測

どちらかのモデルを用いてトレーニングを行う
(ただし、どちらの場合でも新しい単語には対応できない)

## Skip-Gram

参考記事

- [word2vec（Skip-Gram Model）の仕組みを恐らく日本一簡潔にまとめてみたつもり](http://www.randpy.tokyo/entry/word2vec_skip_gram_model)
- [【論文紹介】Distributed Representations of Sentences and Documents](https://www.slideshare.net/TomofumiYoshida2/distributed-representations-of-sentences-and-documents-70011001)

## 処理の流れ

予定

1. word2vec でモデル(学習済みデータ)を用意する
2. 形態素解析で映画のあらすじの文章を単語レベルに分割する
3. 一つの文章ごとに、単語レベルでベクトル化
4. 一つの文章内の全単語ベクトルの平均を算出し、文章ベクトルとする
5. 文章ベクトル同士の cos 類似度を計算し、比較
6. 最も類似度の高い文章を出力する
